{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaaa489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 965.4/965.4 kB 9.0 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pygame\n",
      "  Using cached pygame-2.6.1-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Using cached pygame-2.6.1-cp312-cp312-win_amd64.whl (10.6 MB)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2996c1a",
   "metadata": {},
   "source": [
    "# Carga y exploración de un entorno de Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f1e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import product\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# -------------------- VISUALIZACIÓN DE RESULTADOS --------------------\n",
    "\n",
    "def graficar_recompensas(agente):\n",
    "    \"\"\"Grafica la recompensa media acumulada por episodio.\"\"\"\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(agente.stats)\n",
    "    plt.title('Recompensa media acumulada')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa media')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def graficar_longitud_episodios(agente):\n",
    "    \"\"\"Grafica la longitud de cada episodio.\"\"\"\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(agente.episode_lengths)\n",
    "    plt.title(\"Longitud de episodios\")\n",
    "    plt.xlabel(\"Episodio\")\n",
    "    plt.ylabel(\"Pasos\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def mostrar_resultados_agente_continuo(agente):\n",
    "    \"\"\"Muestra gráficos de rendimiento en entornos continuos.\"\"\"\n",
    "    graficar_recompensas(agente)\n",
    "    graficar_longitud_episodios(agente)\n",
    "\n",
    "# -------------------- EJECUCIÓN DE UN EPISODIO --------------------\n",
    "\n",
    "def ejecutar_episodio_y_mostrar(agente, render=False):\n",
    "    \"\"\"Ejecuta un episodio con la política aprendida y muestra la evolución de la posición.\"\"\"\n",
    "    env = agente.env\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    posiciones = []\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        posiciones.append(state[0])  # Guardamos la posición del coche\n",
    "        action = agente._seleccionar_accion_rbf(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    # Mostrar gráfico de posiciones\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(posiciones)\n",
    "    plt.title(\"Evolución de la posición del coche\")\n",
    "    plt.xlabel(\"Paso del episodio\")\n",
    "    plt.ylabel(\"Posición\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Recompensa total obtenida: {total_reward:.2f}\")\n",
    "\n",
    "# -------------------- VISUALIZACIÓN DE LA POLÍTICA EN EL ESPACIO CONTINUO --------------------\n",
    "\n",
    "def visualizar_politica_aprendida(agente, resolution=50):\n",
    "    \"\"\"Visualiza la política aprendida en el espacio de estados continuo.\"\"\"\n",
    "    x = np.linspace(agente.low[0], agente.high[0], resolution)  # posición\n",
    "    y = np.linspace(agente.low[1], agente.high[1], resolution)  # velocidad\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    \n",
    "    policy_map = np.zeros_like(xx)\n",
    "\n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            state = np.array([xx[i, j], yy[i, j]])\n",
    "            policy_map[i, j] = agente._seleccionar_accion_rbf(state)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, policy_map, levels=agente.nA, cmap=\"coolwarm\", alpha=0.8)\n",
    "    cbar = plt.colorbar(ticks=range(agente.nA))\n",
    "    cbar.ax.set_yticklabels([f\"A{a}\" for a in range(agente.nA)])\n",
    "    plt.xlabel(\"Posición\")\n",
    "    plt.ylabel(\"Velocidad\")\n",
    "    plt.title(\"Política aprendida (acción elegida en cada punto)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a19ac",
   "metadata": {},
   "source": [
    "# Objeto AgenteSARSA-SemiGradiente con RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b7d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFBasisFunctions:\n",
    "    def __init__(self, low, high, num_centers=(5, 5), sigma=0.2):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.num_centers = np.array(num_centers)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Crear centros de las funciones RBF en una grilla regular\n",
    "        pos_centers = np.linspace(low[0], high[0], num_centers[0])\n",
    "        vel_centers = np.linspace(low[1], high[1], num_centers[1])\n",
    "        \n",
    "        self.centers = []\n",
    "        for p in pos_centers:\n",
    "            for v in vel_centers:\n",
    "                self.centers.append([p, v])\n",
    "        \n",
    "        self.centers = np.array(self.centers)\n",
    "        self.num_features = len(self.centers)\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        \"\"\"Calcula las características RBF para un estado dado.\"\"\"\n",
    "        state = np.array(state)\n",
    "        features = np.zeros(self.num_features)\n",
    "        \n",
    "        for i, center in enumerate(self.centers):\n",
    "            distance = np.linalg.norm(state - center)\n",
    "            features[i] = np.exp(-(distance ** 2) / (2 * self.sigma ** 2))\n",
    "        \n",
    "        return features\n",
    "\n",
    "class AgenteSARSARBF:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=True, num_centers=(5, 5), sigma=0.2):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.low = env.observation_space.low\n",
    "        self.high = env.observation_space.high\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "        # Inicializar funciones RBF\n",
    "        self.rbf = RBFBasisFunctions(self.low, self.high, num_centers, sigma)\n",
    "        self.d = self.rbf.num_features\n",
    "\n",
    "        # Una theta por acción\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _phi(self, state):\n",
    "        \"\"\"Devuelve vector de características RBF para un estado\"\"\"\n",
    "        return self.rbf.get_features(state)\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        \"\"\"Valor aproximado Q(s, a)\"\"\"\n",
    "        return np.dot(self.theta[action], self._phi(state))\n",
    "\n",
    "    # -------------------- POLÍTICAS Y SELECCIÓN DE ACCIONES --------------------\n",
    "\n",
    "    def _seleccionar_accion_rbf(self, state):\n",
    "        \"\"\"Selecciona una acción usando la política epsilon-soft explícita.\"\"\"\n",
    "        policy = self._epsilon_soft_policy_rbf(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy_rbf(self, state):\n",
    "        \"\"\"Devuelve una política epsilon-soft como vector de probabilidades para entornos con RBF.\"\"\"\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        \n",
    "        # Manejar valores problemáticos\n",
    "        if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):\n",
    "            q_values = np.nan_to_num(q_values)\n",
    "            \n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        \n",
    "        # Asegurar que las probabilidades son válidas\n",
    "        policy = np.clip(policy, 0, 1)\n",
    "        if policy.sum() == 0:\n",
    "            policy = np.ones(self.nA) / self.nA\n",
    "        else:\n",
    "            policy /= policy.sum()\n",
    "            \n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=True):\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "    \n",
    "        acumulador_recompensas = 0.0\n",
    "    \n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra):\n",
    "            if self.decay:\n",
    "                self.epsilon = min(1.0, 1000.0 / (t + 1))\n",
    "    \n",
    "            state, _ = self.env.reset()\n",
    "            action = self._seleccionar_accion_rbf(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "    \n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion_rbf(next_state)\n",
    "    \n",
    "                phi = self._phi(state)\n",
    "    \n",
    "                # Verificar características válidas\n",
    "                if np.any(np.isnan(phi)) or np.any(np.isinf(phi)):\n",
    "                    phi = np.nan_to_num(phi)\n",
    "    \n",
    "                q_current = self._Q(state, action)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "    \n",
    "                if np.isnan(delta) or np.isinf(delta):\n",
    "                    delta = 0.0\n",
    "    \n",
    "                # Actualización con control de valores extremos\n",
    "                self.theta[action] += self.alpha * delta * phi\n",
    "                self.theta[action] = np.clip(self.theta[action], -1e3, 1e3)\n",
    "    \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "    \n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "    \n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835d2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_configuracion(params, env_name=\"MountainCar-v0\"):\n",
    "    try:\n",
    "        alpha, gamma, epsilon, decay, num_centers, sigma = params\n",
    "        env = gym.make(env_name)\n",
    "        env.reset(seed=SEED)\n",
    "        agente = AgenteSARSARBF(\n",
    "            env, alpha=alpha, gamma=gamma, epsilon=epsilon,\n",
    "            decay=decay, num_centers=num_centers, sigma=sigma\n",
    "        )\n",
    "        agente.entrenar(num_episodes=3000, mostrar_barra=False)\n",
    "        recompensa_final = np.mean(agente.stats[-100:]) if len(agente.stats) >= 100 else np.mean(agente.stats)\n",
    "        env.close()\n",
    "        return (alpha, gamma, epsilon, decay, num_centers, sigma, recompensa_final)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en configuración {params}: {e}\")\n",
    "        return (alpha, gamma, epsilon, decay, num_centers, sigma, -float('inf'))\n",
    "\n",
    "def random_search_secuencial(env_name=\"MountainCar-v0\", n_trials=50):\n",
    "    \"\"\"Versión secuencial del random search para evitar problemas de multiprocesamiento.\"\"\"\n",
    "    # Espacio de búsqueda\n",
    "    alphas = [0.01, 0.05, 0.1, 0.5]\n",
    "    gammas = [0.9, 0.95, 1.0]\n",
    "    epsilons = [0.1, 0.2, 0.3, 0.5]\n",
    "    num_centers_list = [(3, 3), (5, 5), (7, 7), (10, 10)]\n",
    "    sigmas = [0.1, 0.2, 0.5, 1.0]\n",
    "\n",
    "    combinaciones = []\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        a = random.choice(alphas)\n",
    "        g = random.choice(gammas)\n",
    "        nc = random.choice(num_centers_list)\n",
    "        s = random.choice(sigmas)\n",
    "        decay = random.choice([True, False])\n",
    "        if decay:\n",
    "            e = 0.0  # ignorado\n",
    "        else:\n",
    "            e = random.choice(epsilons)\n",
    "        combinaciones.append((a, g, e, decay, nc, s))\n",
    "\n",
    "    mejor_config = None\n",
    "    mejor_recompensa = -float('inf')\n",
    "    resultados = []\n",
    "\n",
    "    print(f\"🔍 Random Search RBF (secuencial): ejecutando {n_trials} configuraciones...\\n\")\n",
    "\n",
    "    for i, combo in enumerate(tqdm(combinaciones, desc=\"Progreso\")):\n",
    "        print(f\"Configuración {i+1}/{n_trials}: α={combo[0]}, γ={combo[1]}, decay={combo[3]}, centers={combo[4]}, σ={combo[5]}\")\n",
    "        \n",
    "        resultado = evaluar_configuracion(combo, env_name)\n",
    "        alpha, gamma, epsilon, decay_flag, num_centers, sigma, recompensa = resultado\n",
    "        resultados.append(resultado)\n",
    "\n",
    "        if recompensa > mejor_recompensa:\n",
    "            mejor_recompensa = recompensa\n",
    "            mejor_config = (alpha, gamma, epsilon, decay_flag, num_centers, sigma)\n",
    "            print(f\"  ✅ Nueva mejor configuración! Recompensa: {mejor_recompensa:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ MEJOR CONFIGURACIÓN ENCONTRADA:\")\n",
    "    print(f\" alpha = {mejor_config[0]}\")\n",
    "    print(f\" γ = {mejor_config[1]}\")\n",
    "    print(f\" ε = {mejor_config[2]}\")\n",
    "    print(f\" decay = {mejor_config[3]}\")\n",
    "    print(f\" num_centers = {mejor_config[4]}\")\n",
    "    print(f\" sigma = {mejor_config[5]}\")\n",
    "    print(f\"  → Recompensa media final: {mejor_recompensa:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return mejor_config, mejor_recompensa, resultados\n",
    "\n",
    "def random_search(env_name=\"MountainCar-v0\", n_trials=30):\n",
    "    \"\"\"Wrapper que llama a la versión secuencial.\"\"\"\n",
    "    return random_search_secuencial(env_name, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82710e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando búsqueda de hiperparámetros para SARSA RBF...\n",
      "🔍 Random Search RBF (secuencial): ejecutando 30 configuraciones...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración 1/30: α=0.01, γ=0.9, decay=True, centers=(7, 7), σ=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso:   3%|▎         | 1/30 [54:10<26:11:02, 3250.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Nueva mejor configuración! Recompensa: -200.0000\n",
      "Configuración 2/30: α=0.05, γ=1.0, decay=False, centers=(3, 3), σ=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso:   7%|▋         | 2/30 [1:06:41<13:50:38, 1779.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Nueva mejor configuración! Recompensa: -178.2809\n",
      "Configuración 3/30: α=0.01, γ=0.9, decay=True, centers=(5, 5), σ=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progreso:  10%|█         | 3/30 [1:34:17<12:55:33, 1723.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración 4/30: α=0.05, γ=1.0, decay=False, centers=(10, 10), σ=0.2\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar búsqueda de hiperparámetros\n",
    "print(\"🚀 Iniciando búsqueda de hiperparámetros para SARSA RBF...\")\n",
    "mejor_config, mejor_recompensa, resultados = random_search(n_trials=30)\n",
    "\n",
    "print(f\"\\n📊 Mejores hiperparámetros encontrados:\")\n",
    "print(f\"   α = {mejor_config[0]}\")\n",
    "print(f\"   γ = {mejor_config[1]}\")  \n",
    "print(f\"   ε = {mejor_config[2]}\")\n",
    "print(f\"   decay = {mejor_config[3]}\")\n",
    "print(f\"   num_centers = {mejor_config[4]}\")\n",
    "print(f\"   σ = {mejor_config[5]}\")\n",
    "print(f\"   Recompensa final = {mejor_recompensa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c07a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución de ejemplo\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset(seed=SEED)\n",
    "agente = AgenteSARSARBF(env, alpha=0.1, gamma=1.0, epsilon=0.3, num_centers=(7, 7), sigma=0.2, decay=False)\n",
    "agente.entrenar(num_episodes=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86e0b1",
   "metadata": {},
   "source": [
    "# Resultados de agente SARSA SemiGradiente con RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf54fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostrar_resultados_agente_continuo(agente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20170c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ejecutar_episodio_y_mostrar(agente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ee9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabar_video_agente(agente, nombre_archivo=\"video_mountaincar.gif\", fps=30):\n",
    "    \"\"\"\n",
    "    Ejecuta un episodio con la política aprendida y guarda un video del entorno.\n",
    "    \"\"\"\n",
    "    # Crear entorno con renderizado de imágenes\n",
    "    env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "    env.reset(seed=SEED)\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    frames = []\n",
    "\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "\n",
    "        # Acción greedy\n",
    "        q_values = [agente._Q(state, a) for a in range(agente.nA)]\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(f\"Número total de frames: {len(frames)}\")\n",
    "    # Guardar el video como GIF\n",
    "    imageio.mimsave(nombre_archivo, frames, fps=fps, loop=0)\n",
    "    print(f\"🎥 Vídeo guardado en: {nombre_archivo}\")\n",
    "    print(f\"🏁 Recompensa total obtenida: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28dbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = \"video_mountaincar_rbf.gif\"\n",
    "\n",
    "grabar_video_agente(agente, nombre_archivo=nombre_archivo)\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<img src=\"{nombre_archivo}\" style=\"width: 600px;\" loop>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ee270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
