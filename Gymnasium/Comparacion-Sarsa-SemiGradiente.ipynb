{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559a24b9",
   "metadata": {},
   "source": [
    "# Comparación de Métodos SARSA Semi-gradiente en MountainCar\n",
    "\n",
    "Este notebook compara tres implementaciones de SARSA semi-gradiente con diferentes representaciones de características:\n",
    "1. **Fourier Basis Functions**\n",
    "2. **Tile Coding**\n",
    "3. **Radial Basis Functions (RBF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4ca1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygame in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5ba253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import product\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------- IMPLEMENTACIONES DE LOS AGENTES --------------------\n",
    "\n",
    "# Agente Fourier\n",
    "class AgenteSARSASemiGradienteFourier:\n",
    "    def __init__(self, env, alpha=0.01, gamma=1.0, epsilon=1.0, decay=True, fourier_order=3):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.fourier_order = fourier_order\n",
    "        self.name = \"Fourier\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.low = env.observation_space.low\n",
    "        self.high = env.observation_space.high\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "        self.c = np.array(list(product(range(fourier_order + 1), repeat=self.state_dim)))\n",
    "        self.d = len(self.c)\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _scale_state(self, state):\n",
    "        return (state - self.low) / (self.high - self.low)\n",
    "\n",
    "    def _phi(self, state):\n",
    "        s_scaled = self._scale_state(state)\n",
    "        return np.cos(np.pi * np.dot(self.c, s_scaled))\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        return np.dot(self.theta[action], self._phi(state))\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = min(1.0, 1000.0 / (t + 1))\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self._phi(state)\n",
    "                if np.any(np.isnan(phi)) or np.any(np.isinf(phi)):\n",
    "                    phi = np.nan_to_num(phi)\n",
    "\n",
    "                q_current = self._Q(state, action)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if np.isnan(delta) or np.isinf(delta):\n",
    "                    delta = 0.0\n",
    "\n",
    "                self.theta[action] += self.alpha * delta * phi\n",
    "                self.theta[action] = np.clip(self.theta[action], -1e3, 1e3)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta\n",
    "\n",
    "# Agente Tile Coding\n",
    "class TileCoder:\n",
    "    def __init__(self, low, high, bins=(10, 10), num_tilings=8):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.bins = np.array(bins)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tile_width = (self.high - self.low) / (self.bins - 1)\n",
    "        self.total_tiles = np.prod(self.bins) * num_tilings\n",
    "        self.offsets = [\n",
    "            np.random.uniform(0, self.tile_width, size=len(self.low))\n",
    "            for _ in range(num_tilings)\n",
    "        ]\n",
    "\n",
    "    def get_features(self, state):\n",
    "        features = np.zeros(self.total_tiles)\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            shifted = np.array(state) + offset\n",
    "            idx = ((shifted - self.low) / self.tile_width).astype(int)\n",
    "            idx = np.clip(idx, 0, self.bins - 1)\n",
    "            tile_index = np.ravel_multi_index(idx, self.bins)\n",
    "            feature_index = i * np.prod(self.bins) + tile_index\n",
    "            features[int(feature_index)] = 1\n",
    "        return features\n",
    "\n",
    "class AgenteSARSATileCoding:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=True, bins=(8, 8), num_tilings=8):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.name = \"Tile Coding\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.tile_coder = TileCoder(env.observation_space.low, env.observation_space.high, bins, num_tilings)\n",
    "        self.d = self.tile_coder.total_tiles\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        phi = self.tile_coder.get_features(state)\n",
    "        return np.dot(self.theta[action], phi)\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):\n",
    "            q_values = np.nan_to_num(q_values)\n",
    "        \n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        \n",
    "        policy = np.clip(policy, 0, 1)\n",
    "        if policy.sum() == 0:\n",
    "            policy = np.ones(self.nA) / self.nA\n",
    "        else:\n",
    "            policy /= policy.sum()\n",
    "        \n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = max(0.05, 1000.0 / (t + 1))\n",
    "\n",
    "            state, _ = self.env.reset(seed=42)\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self.tile_coder.get_features(state)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                q_current = np.dot(self.theta[action], phi)\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if not (np.isnan(delta) or np.isinf(delta)):\n",
    "                    self.theta[action] += self.alpha * delta * phi\n",
    "                    self.theta[action] = np.clip(self.theta[action], -1e6, 1e6)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta\n",
    "\n",
    "# Agente RBF\n",
    "class RBFBasisFunctions:\n",
    "    def __init__(self, low, high, num_centers=(5, 5), sigma=0.2):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.num_centers = np.array(num_centers)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        pos_centers = np.linspace(low[0], high[0], num_centers[0])\n",
    "        vel_centers = np.linspace(low[1], high[1], num_centers[1])\n",
    "        \n",
    "        self.centers = []\n",
    "        for p in pos_centers:\n",
    "            for v in vel_centers:\n",
    "                self.centers.append([p, v])\n",
    "        \n",
    "        self.centers = np.array(self.centers)\n",
    "        self.num_features = len(self.centers)\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        state = np.array(state)\n",
    "        features = np.zeros(self.num_features)\n",
    "        \n",
    "        for i, center in enumerate(self.centers):\n",
    "            distance = np.linalg.norm(state - center)\n",
    "            features[i] = np.exp(-(distance ** 2) / (2 * self.sigma ** 2))\n",
    "        \n",
    "        return features\n",
    "\n",
    "class AgenteSARSARBF:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=True, num_centers=(5, 5), sigma=0.2):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.name = \"RBF\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.low = env.observation_space.low\n",
    "        self.high = env.observation_space.high\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "        self.rbf = RBFBasisFunctions(self.low, self.high, num_centers, sigma)\n",
    "        self.d = self.rbf.num_features\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _phi(self, state):\n",
    "        return self.rbf.get_features(state)\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        return np.dot(self.theta[action], self._phi(state))\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        \n",
    "        if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):\n",
    "            q_values = np.nan_to_num(q_values)\n",
    "            \n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        \n",
    "        policy = np.clip(policy, 0, 1)\n",
    "        if policy.sum() == 0:\n",
    "            policy = np.ones(self.nA) / self.nA\n",
    "        else:\n",
    "            policy /= policy.sum()\n",
    "            \n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = min(1.0, 1000.0 / (t + 1))\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self._phi(state)\n",
    "                if np.any(np.isnan(phi)) or np.any(np.isinf(phi)):\n",
    "                    phi = np.nan_to_num(phi)\n",
    "\n",
    "                q_current = self._Q(state, action)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if np.isnan(delta) or np.isinf(delta):\n",
    "                    delta = 0.0\n",
    "\n",
    "                self.theta[action] += self.alpha * delta * phi\n",
    "                self.theta[action] = np.clip(self.theta[action], -1e3, 1e3)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c3863",
   "metadata": {},
   "source": [
    "# Comparación de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando comparación de métodos SARSA Semi-gradiente...\n",
      "\n",
      "🔄 Ejecutando run 1/3\n",
      "  Entrenando Fourier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando Fourier: 100%|██████████| 8000/8000 [09:46<00:00, 13.63it/s]\n",
      "Entrenando Fourier: 100%|██████████| 8000/8000 [09:46<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entrenando Tile Coding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando Tile Coding: 100%|██████████| 8000/8000 [1:01:50<00:00,  2.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entrenando RBF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando RBF:  21%|██▏       | 1714/8000 [28:29<2:40:36,  1.53s/it]"
     ]
    }
   ],
   "source": [
    "def comparar_agentes(num_episodes=10000, num_runs=5):\n",
    "    \"\"\"Compara los tres métodos ejecutando múltiples runs y calculando estadísticas.\"\"\"\n",
    "    \n",
    "    # Configuraciones optimizadas para cada método\n",
    "    configs = {\n",
    "        'Fourier': {'alpha': 0.01, 'gamma': 1.0, 'epsilon': 0.3, 'fourier_order': 5, 'decay': False},\n",
    "        'Tile Coding': {'alpha': 0.01, 'gamma': 1.0, 'epsilon': 0.1, 'decay': False, 'bins': (8, 8), 'num_tilings': 8},\n",
    "        'RBF': {'alpha': 0.1, 'gamma': 1.0, 'epsilon': 0.3, 'num_centers': (7, 7), 'sigma': 0.2, 'decay': False}\n",
    "    }\n",
    "    \n",
    "    resultados = {metodo: {'recompensas': [], 'longitudes': [], 'tiempos': []} for metodo in configs.keys()}\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n🔄 Ejecutando run {run + 1}/{num_runs}\")\n",
    "        \n",
    "        for metodo, config in configs.items():\n",
    "            print(f\"  Entrenando {metodo}...\")\n",
    "            env = gym.make(\"MountainCar-v0\")\n",
    "            env.reset(seed=SEED + run)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            if metodo == 'Fourier':\n",
    "                agente = AgenteSARSASemiGradienteFourier(env, **config)\n",
    "            elif metodo == 'Tile Coding':\n",
    "                agente = AgenteSARSATileCoding(env, **config)\n",
    "            elif metodo == 'RBF':\n",
    "                agente = AgenteSARSARBF(env, **config)\n",
    "            \n",
    "            agente.entrenar(num_episodes=num_episodes, mostrar_barra=True)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            resultados[metodo]['recompensas'].append(agente.stats)\n",
    "            resultados[metodo]['longitudes'].append(agente.episode_lengths)\n",
    "            resultados[metodo]['tiempos'].append(end_time - start_time)\n",
    "            \n",
    "            env.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar comparación\n",
    "print(\"🚀 Iniciando comparación de métodos SARSA Semi-gradiente...\")\n",
    "resultados = comparar_agentes(num_episodes=8000, num_runs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_comparacion(resultados):\n",
    "    \"\"\"Visualiza los resultados de la comparación.\"\"\"\n",
    "    \n",
    "    # 1. Recompensas promedio a lo largo del entrenamiento\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    for metodo in resultados.keys():\n",
    "        recompensas_runs = resultados[metodo]['recompensas']\n",
    "        recompensas_mean = np.mean(recompensas_runs, axis=0)\n",
    "        recompensas_std = np.std(recompensas_runs, axis=0)\n",
    "        \n",
    "        episodes = range(len(recompensas_mean))\n",
    "        plt.plot(episodes, recompensas_mean, label=metodo, linewidth=2)\n",
    "        plt.fill_between(episodes, \n",
    "                        recompensas_mean - recompensas_std, \n",
    "                        recompensas_mean + recompensas_std, \n",
    "                        alpha=0.2)\n",
    "    \n",
    "    plt.title('Recompensa Media Acumulada')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Media')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. Longitud de episodios promedio\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for metodo in resultados.keys():\n",
    "        longitudes_runs = resultados[metodo]['longitudes']\n",
    "        longitudes_mean = np.mean(longitudes_runs, axis=0)\n",
    "        longitudes_std = np.std(longitudes_runs, axis=0)\n",
    "        \n",
    "        episodes = range(len(longitudes_mean))\n",
    "        plt.plot(episodes, longitudes_mean, label=metodo, linewidth=2)\n",
    "        plt.fill_between(episodes, \n",
    "                        longitudes_mean - longitudes_std, \n",
    "                        longitudes_mean + longitudes_std, \n",
    "                        alpha=0.2)\n",
    "    \n",
    "    plt.title('Longitud Media de Episodios')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Pasos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. Tiempo de entrenamiento\n",
    "    plt.subplot(1, 3, 3)\n",
    "    metodos = list(resultados.keys())\n",
    "    tiempos_mean = [np.mean(resultados[m]['tiempos']) for m in metodos]\n",
    "    tiempos_std = [np.std(resultados[m]['tiempos']) for m in metodos]\n",
    "    \n",
    "    bars = plt.bar(metodos, tiempos_mean, yerr=tiempos_std, capsize=5, alpha=0.7)\n",
    "    plt.title('Tiempo de Entrenamiento')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bar, tiempo in zip(bars, tiempos_mean):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + tiempo*0.01, \n",
    "                f'{tiempo:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualizar_comparacion(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tabla_resumen(resultados):\n",
    "    \"\"\"Crea una tabla resumen con las métricas de cada método.\"\"\"\n",
    "    \n",
    "    metricas = []\n",
    "    \n",
    "    for metodo in resultados.keys():\n",
    "        # Recompensa final promedio (últimos 100 episodios)\n",
    "        recompensas_finales = [np.mean(run[-100:]) for run in resultados[metodo]['recompensas']]\n",
    "        recompensa_final_mean = np.mean(recompensas_finales)\n",
    "        recompensa_final_std = np.std(recompensas_finales)\n",
    "        \n",
    "        # Longitud final promedio (últimos 100 episodios)\n",
    "        longitudes_finales = [np.mean(run[-100:]) for run in resultados[metodo]['longitudes']]\n",
    "        longitud_final_mean = np.mean(longitudes_finales)\n",
    "        longitud_final_std = np.std(longitudes_finales)\n",
    "        \n",
    "        # Tiempo promedio\n",
    "        tiempo_mean = np.mean(resultados[metodo]['tiempos'])\n",
    "        tiempo_std = np.std(resultados[metodo]['tiempos'])\n",
    "        \n",
    "        # Episodios para converger (cuando la longitud media móvil < 150)\n",
    "        convergencia_episodios = []\n",
    "        for run_longitudes in resultados[metodo]['longitudes']:\n",
    "            ventana = 100\n",
    "            medias_moviles = [np.mean(run_longitudes[i:i+ventana]) for i in range(len(run_longitudes)-ventana)]\n",
    "            try:\n",
    "                episodio_convergencia = next(i for i, media in enumerate(medias_moviles) if media < 150)\n",
    "                convergencia_episodios.append(episodio_convergencia)\n",
    "            except StopIteration:\n",
    "                convergencia_episodios.append(len(run_longitudes))\n",
    "        \n",
    "        convergencia_mean = np.mean(convergencia_episodios)\n",
    "        convergencia_std = np.std(convergencia_episodios)\n",
    "        \n",
    "        metricas.append({\n",
    "            'Método': metodo,\n",
    "            'Recompensa Final': f\"{recompensa_final_mean:.2f} ± {recompensa_final_std:.2f}\",\n",
    "            'Longitud Final': f\"{longitud_final_mean:.1f} ± {longitud_final_std:.1f}\",\n",
    "            'Tiempo (s)': f\"{tiempo_mean:.1f} ± {tiempo_std:.1f}\",\n",
    "            'Episodios a Convergencia': f\"{convergencia_mean:.0f} ± {convergencia_std:.0f}\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metricas)\n",
    "    return df\n",
    "\n",
    "tabla_resumen = crear_tabla_resumen(resultados)\n",
    "print(\"📊 Tabla Resumen de Resultados:\")\n",
    "print(\"=\"*80)\n",
    "print(tabla_resumen.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbac19",
   "metadata": {},
   "source": [
    "# Análisis de Características\n",
    "\n",
    "Vamos a analizar las características de cada método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_caracteristicas():\n",
    "    \"\"\"Analiza las características de cada método de aproximación.\"\"\"\n",
    "    \n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    # Crear agentes con configuraciones típicas\n",
    "    agente_fourier = AgenteSARSASemiGradienteFourier(env, fourier_order=3)\n",
    "    agente_tile = AgenteSARSATileCoding(env, bins=(8, 8), num_tilings=8)\n",
    "    agente_rbf = AgenteSARSARBF(env, num_centers=(5, 5), sigma=0.2)\n",
    "    \n",
    "    # Estado de ejemplo\n",
    "    state_ejemplo = np.array([-0.5, 0.0])  # posición media, velocidad cero\n",
    "    \n",
    "    # Obtener características\n",
    "    phi_fourier = agente_fourier._phi(state_ejemplo)\n",
    "    phi_tile = agente_tile.tile_coder.get_features(state_ejemplo)\n",
    "    phi_rbf = agente_rbf._phi(state_ejemplo)\n",
    "    \n",
    "    print(\"🔍 Análisis de Características para estado [-0.5, 0.0]:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Fourier (orden {agente_fourier.fourier_order}):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_fourier)}\")\n",
    "    print(f\"  - Características activas: {np.sum(phi_fourier != 0)}\")\n",
    "    print(f\"  - Rango de valores: [{phi_fourier.min():.3f}, {phi_fourier.max():.3f}]\")\n",
    "    print(f\"  - Características top-5: {phi_fourier[:5]}\")\n",
    "    \n",
    "    print(f\"\\nTile Coding ({agente_tile.tile_coder.bins} bins, {agente_tile.tile_coder.num_tilings} tilings):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_tile)}\")\n",
    "    print(f\"  - Características activas: {np.sum(phi_tile != 0)}\")\n",
    "    print(f\"  - Características binarias: {np.all(np.isin(phi_tile, [0, 1]))}\")\n",
    "    print(f\"  - Índices activos: {np.where(phi_tile == 1)[0]}\")\n",
    "    \n",
    "    print(f\"\\nRBF ({agente_rbf.rbf.num_centers} centros, σ={agente_rbf.rbf.sigma}):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_rbf)}\")\n",
    "    print(f\"  - Características activas: {np.sum(phi_rbf > 0.01)}\")\n",
    "    print(f\"  - Rango de valores: [{phi_rbf.min():.3f}, {phi_rbf.max():.3f}]\")\n",
    "    print(f\"  - Características top-5: {phi_rbf[:5]}\")\n",
    "    \n",
    "    # Visualizar características\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Fourier\n",
    "    axes[0].bar(range(len(phi_fourier)), phi_fourier, alpha=0.7)\n",
    "    axes[0].set_title(f'Características Fourier (dim={len(phi_fourier)})')\n",
    "    axes[0].set_xlabel('Índice de característica')\n",
    "    axes[0].set_ylabel('Valor')\n",
    "    \n",
    "    # Tile Coding\n",
    "    indices_activos = np.where(phi_tile == 1)[0]\n",
    "    if len(indices_activos) > 0:\n",
    "        axes[1].bar(indices_activos, phi_tile[indices_activos], alpha=0.7, color='orange')\n",
    "    axes[1].set_title(f'Características Tile Coding (dim={len(phi_tile)})')\n",
    "    axes[1].set_xlabel('Índice de característica')\n",
    "    axes[1].set_ylabel('Valor')\n",
    "    \n",
    "    # RBF\n",
    "    axes[2].bar(range(len(phi_rbf)), phi_rbf, alpha=0.7, color='green')\n",
    "    axes[2].set_title(f'Características RBF (dim={len(phi_rbf)})')\n",
    "    axes[2].set_xlabel('Índice de característica')\n",
    "    axes[2].set_ylabel('Valor')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "analizar_caracteristicas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a14eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
