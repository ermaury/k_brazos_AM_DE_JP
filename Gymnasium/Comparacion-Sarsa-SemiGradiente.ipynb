{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559a24b9",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de M√©todos SARSA Semi-gradiente en MountainCar\n",
    "\n",
    "Este notebook compara tres implementaciones de SARSA semi-gradiente con diferentes representaciones de caracter√≠sticas:\n",
    "1. **Fourier Basis Functions**\n",
    "2. **Tile Coding**\n",
    "3. **Radial Basis Functions (RBF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4ca1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gymnasium in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from gymnasium) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pygame in c:\\users\\dario\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\dario\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5ba253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import product\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# -------------------- IMPLEMENTACIONES DE LOS AGENTES --------------------\n",
    "\n",
    "# Agente Fourier\n",
    "class AgenteSARSASemiGradienteFourier:\n",
    "    def __init__(self, env, alpha=0.01, gamma=1.0, epsilon=1.0, decay=True, fourier_order=3):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.fourier_order = fourier_order\n",
    "        self.name = \"Fourier\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.low = env.observation_space.low\n",
    "        self.high = env.observation_space.high\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "        self.c = np.array(list(product(range(fourier_order + 1), repeat=self.state_dim)))\n",
    "        self.d = len(self.c)\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _scale_state(self, state):\n",
    "        return (state - self.low) / (self.high - self.low)\n",
    "\n",
    "    def _phi(self, state):\n",
    "        s_scaled = self._scale_state(state)\n",
    "        return np.cos(np.pi * np.dot(self.c, s_scaled))\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        return np.dot(self.theta[action], self._phi(state))\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = min(1.0, 1000.0 / (t + 1))\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self._phi(state)\n",
    "                if np.any(np.isnan(phi)) or np.any(np.isinf(phi)):\n",
    "                    phi = np.nan_to_num(phi)\n",
    "\n",
    "                q_current = self._Q(state, action)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if np.isnan(delta) or np.isinf(delta):\n",
    "                    delta = 0.0\n",
    "\n",
    "                self.theta[action] += self.alpha * delta * phi\n",
    "                self.theta[action] = np.clip(self.theta[action], -1e3, 1e3)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta\n",
    "\n",
    "# Agente Tile Coding\n",
    "class TileCoder:\n",
    "    def __init__(self, low, high, bins=(10, 10), num_tilings=8):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.bins = np.array(bins)\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tile_width = (self.high - self.low) / (self.bins - 1)\n",
    "        self.total_tiles = np.prod(self.bins) * num_tilings\n",
    "        self.offsets = [\n",
    "            np.random.uniform(0, self.tile_width, size=len(self.low))\n",
    "            for _ in range(num_tilings)\n",
    "        ]\n",
    "\n",
    "    def get_features(self, state):\n",
    "        features = np.zeros(self.total_tiles)\n",
    "        for i, offset in enumerate(self.offsets):\n",
    "            shifted = np.array(state) + offset\n",
    "            idx = ((shifted - self.low) / self.tile_width).astype(int)\n",
    "            idx = np.clip(idx, 0, self.bins - 1)\n",
    "            tile_index = np.ravel_multi_index(idx, self.bins)\n",
    "            feature_index = i * np.prod(self.bins) + tile_index\n",
    "            features[int(feature_index)] = 1\n",
    "        return features\n",
    "\n",
    "class AgenteSARSATileCoding:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=True, bins=(8, 8), num_tilings=8):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.name = \"Tile Coding\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.tile_coder = TileCoder(env.observation_space.low, env.observation_space.high, bins, num_tilings)\n",
    "        self.d = self.tile_coder.total_tiles\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        phi = self.tile_coder.get_features(state)\n",
    "        return np.dot(self.theta[action], phi)\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):\n",
    "            q_values = np.nan_to_num(q_values)\n",
    "        \n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        \n",
    "        policy = np.clip(policy, 0, 1)\n",
    "        if policy.sum() == 0:\n",
    "            policy = np.ones(self.nA) / self.nA\n",
    "        else:\n",
    "            policy /= policy.sum()\n",
    "        \n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = max(0.05, 1000.0 / (t + 1))\n",
    "\n",
    "            state, _ = self.env.reset(seed=42)\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self.tile_coder.get_features(state)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                q_current = np.dot(self.theta[action], phi)\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if not (np.isnan(delta) or np.isinf(delta)):\n",
    "                    self.theta[action] += self.alpha * delta * phi\n",
    "                    self.theta[action] = np.clip(self.theta[action], -1e6, 1e6)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta\n",
    "\n",
    "# Agente RBF\n",
    "class RBFBasisFunctions:\n",
    "    def __init__(self, low, high, num_centers=(5, 5), sigma=0.2):\n",
    "        self.low = np.array(low)\n",
    "        self.high = np.array(high)\n",
    "        self.num_centers = np.array(num_centers)\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        pos_centers = np.linspace(low[0], high[0], num_centers[0])\n",
    "        vel_centers = np.linspace(low[1], high[1], num_centers[1])\n",
    "        \n",
    "        self.centers = []\n",
    "        for p in pos_centers:\n",
    "            for v in vel_centers:\n",
    "                self.centers.append([p, v])\n",
    "        \n",
    "        self.centers = np.array(self.centers)\n",
    "        self.num_features = len(self.centers)\n",
    "    \n",
    "    def get_features(self, state):\n",
    "        state = np.array(state)\n",
    "        features = np.zeros(self.num_features)\n",
    "        \n",
    "        for i, center in enumerate(self.centers):\n",
    "            distance = np.linalg.norm(state - center)\n",
    "            features[i] = np.exp(-(distance ** 2) / (2 * self.sigma ** 2))\n",
    "        \n",
    "        return features\n",
    "\n",
    "class AgenteSARSARBF:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, decay=True, num_centers=(5, 5), sigma=0.2):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        self.name = \"RBF\"\n",
    "\n",
    "        self.nA = env.action_space.n\n",
    "        self.low = env.observation_space.low\n",
    "        self.high = env.observation_space.high\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "\n",
    "        self.rbf = RBFBasisFunctions(self.low, self.high, num_centers, sigma)\n",
    "        self.d = self.rbf.num_features\n",
    "        self.theta = np.zeros((self.nA, self.d))\n",
    "        self.stats = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _phi(self, state):\n",
    "        return self.rbf.get_features(state)\n",
    "\n",
    "    def _Q(self, state, action):\n",
    "        return np.dot(self.theta[action], self._phi(state))\n",
    "\n",
    "    def _seleccionar_accion(self, state):\n",
    "        policy = self._epsilon_soft_policy(state)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def _epsilon_soft_policy(self, state):\n",
    "        q_values = np.array([self._Q(state, a) for a in range(self.nA)])\n",
    "        \n",
    "        if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):\n",
    "            q_values = np.nan_to_num(q_values)\n",
    "            \n",
    "        policy = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] += 1.0 - self.epsilon\n",
    "        \n",
    "        policy = np.clip(policy, 0, 1)\n",
    "        if policy.sum() == 0:\n",
    "            policy = np.ones(self.nA) / self.nA\n",
    "        else:\n",
    "            policy /= policy.sum()\n",
    "            \n",
    "        return policy\n",
    "\n",
    "    def entrenar(self, num_episodes=5000, mostrar_barra=False):\n",
    "        acumulador_recompensas = 0.0\n",
    "        for t in tqdm(range(num_episodes), disable=not mostrar_barra, desc=f\"Entrenando {self.name}\"):\n",
    "            if self.decay:\n",
    "                self.epsilon = min(1.0, 1000.0 / (t + 1))\n",
    "            \n",
    "            state, _ = self.env.reset()\n",
    "            action = self._seleccionar_accion(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            pasos = 0\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                next_action = self._seleccionar_accion(next_state)\n",
    "\n",
    "                phi = self._phi(state)\n",
    "                if np.any(np.isnan(phi)) or np.any(np.isinf(phi)):\n",
    "                    phi = np.nan_to_num(phi)\n",
    "\n",
    "                q_current = self._Q(state, action)\n",
    "                q_next = self._Q(next_state, next_action) if not done else 0.0\n",
    "                target = reward + self.gamma * q_next\n",
    "                delta = target - q_current\n",
    "\n",
    "                if np.isnan(delta) or np.isinf(delta):\n",
    "                    delta = 0.0\n",
    "\n",
    "                self.theta[action] += self.alpha * delta * phi\n",
    "                self.theta[action] = np.clip(self.theta[action], -1e3, 1e3)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                total_reward += reward\n",
    "                pasos += 1\n",
    "\n",
    "            self.episode_lengths.append(pasos)\n",
    "            acumulador_recompensas += total_reward\n",
    "            self.stats.append(acumulador_recompensas / (t + 1))\n",
    "\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c3863",
   "metadata": {},
   "source": [
    "# Comparaci√≥n de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando comparaci√≥n de m√©todos SARSA Semi-gradiente...\n",
      "\n",
      "üîÑ Ejecutando run 1/3\n",
      "  Entrenando Fourier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando Fourier: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [09:46<00:00, 13.63it/s]\n",
      "Entrenando Fourier: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [09:46<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entrenando Tile Coding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando Tile Coding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [1:01:50<00:00,  2.16it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entrenando RBF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando RBF:  21%|‚ñà‚ñà‚ñè       | 1714/8000 [28:29<2:40:36,  1.53s/it]"
     ]
    }
   ],
   "source": [
    "def comparar_agentes(num_episodes=10000, num_runs=5):\n",
    "    \"\"\"Compara los tres m√©todos ejecutando m√∫ltiples runs y calculando estad√≠sticas.\"\"\"\n",
    "    \n",
    "    # Configuraciones optimizadas para cada m√©todo\n",
    "    configs = {\n",
    "        'Fourier': {'alpha': 0.01, 'gamma': 1.0, 'epsilon': 0.3, 'fourier_order': 5, 'decay': False},\n",
    "        'Tile Coding': {'alpha': 0.01, 'gamma': 1.0, 'epsilon': 0.1, 'decay': False, 'bins': (8, 8), 'num_tilings': 8},\n",
    "        'RBF': {'alpha': 0.1, 'gamma': 1.0, 'epsilon': 0.3, 'num_centers': (7, 7), 'sigma': 0.2, 'decay': False}\n",
    "    }\n",
    "    \n",
    "    resultados = {metodo: {'recompensas': [], 'longitudes': [], 'tiempos': []} for metodo in configs.keys()}\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\nüîÑ Ejecutando run {run + 1}/{num_runs}\")\n",
    "        \n",
    "        for metodo, config in configs.items():\n",
    "            print(f\"  Entrenando {metodo}...\")\n",
    "            env = gym.make(\"MountainCar-v0\")\n",
    "            env.reset(seed=SEED + run)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            if metodo == 'Fourier':\n",
    "                agente = AgenteSARSASemiGradienteFourier(env, **config)\n",
    "            elif metodo == 'Tile Coding':\n",
    "                agente = AgenteSARSATileCoding(env, **config)\n",
    "            elif metodo == 'RBF':\n",
    "                agente = AgenteSARSARBF(env, **config)\n",
    "            \n",
    "            agente.entrenar(num_episodes=num_episodes, mostrar_barra=True)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            resultados[metodo]['recompensas'].append(agente.stats)\n",
    "            resultados[metodo]['longitudes'].append(agente.episode_lengths)\n",
    "            resultados[metodo]['tiempos'].append(end_time - start_time)\n",
    "            \n",
    "            env.close()\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "print(\"üöÄ Iniciando comparaci√≥n de m√©todos SARSA Semi-gradiente...\")\n",
    "resultados = comparar_agentes(num_episodes=8000, num_runs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_comparacion(resultados):\n",
    "    \"\"\"Visualiza los resultados de la comparaci√≥n.\"\"\"\n",
    "    \n",
    "    # 1. Recompensas promedio a lo largo del entrenamiento\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    for metodo in resultados.keys():\n",
    "        recompensas_runs = resultados[metodo]['recompensas']\n",
    "        recompensas_mean = np.mean(recompensas_runs, axis=0)\n",
    "        recompensas_std = np.std(recompensas_runs, axis=0)\n",
    "        \n",
    "        episodes = range(len(recompensas_mean))\n",
    "        plt.plot(episodes, recompensas_mean, label=metodo, linewidth=2)\n",
    "        plt.fill_between(episodes, \n",
    "                        recompensas_mean - recompensas_std, \n",
    "                        recompensas_mean + recompensas_std, \n",
    "                        alpha=0.2)\n",
    "    \n",
    "    plt.title('Recompensa Media Acumulada')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Media')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 2. Longitud de episodios promedio\n",
    "    plt.subplot(1, 3, 2)\n",
    "    for metodo in resultados.keys():\n",
    "        longitudes_runs = resultados[metodo]['longitudes']\n",
    "        longitudes_mean = np.mean(longitudes_runs, axis=0)\n",
    "        longitudes_std = np.std(longitudes_runs, axis=0)\n",
    "        \n",
    "        episodes = range(len(longitudes_mean))\n",
    "        plt.plot(episodes, longitudes_mean, label=metodo, linewidth=2)\n",
    "        plt.fill_between(episodes, \n",
    "                        longitudes_mean - longitudes_std, \n",
    "                        longitudes_mean + longitudes_std, \n",
    "                        alpha=0.2)\n",
    "    \n",
    "    plt.title('Longitud Media de Episodios')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Pasos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 3. Tiempo de entrenamiento\n",
    "    plt.subplot(1, 3, 3)\n",
    "    metodos = list(resultados.keys())\n",
    "    tiempos_mean = [np.mean(resultados[m]['tiempos']) for m in metodos]\n",
    "    tiempos_std = [np.std(resultados[m]['tiempos']) for m in metodos]\n",
    "    \n",
    "    bars = plt.bar(metodos, tiempos_mean, yerr=tiempos_std, capsize=5, alpha=0.7)\n",
    "    plt.title('Tiempo de Entrenamiento')\n",
    "    plt.ylabel('Tiempo (segundos)')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # A√±adir valores en las barras\n",
    "    for bar, tiempo in zip(bars, tiempos_mean):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + tiempo*0.01, \n",
    "                f'{tiempo:.1f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualizar_comparacion(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_tabla_resumen(resultados):\n",
    "    \"\"\"Crea una tabla resumen con las m√©tricas de cada m√©todo.\"\"\"\n",
    "    \n",
    "    metricas = []\n",
    "    \n",
    "    for metodo in resultados.keys():\n",
    "        # Recompensa final promedio (√∫ltimos 100 episodios)\n",
    "        recompensas_finales = [np.mean(run[-100:]) for run in resultados[metodo]['recompensas']]\n",
    "        recompensa_final_mean = np.mean(recompensas_finales)\n",
    "        recompensa_final_std = np.std(recompensas_finales)\n",
    "        \n",
    "        # Longitud final promedio (√∫ltimos 100 episodios)\n",
    "        longitudes_finales = [np.mean(run[-100:]) for run in resultados[metodo]['longitudes']]\n",
    "        longitud_final_mean = np.mean(longitudes_finales)\n",
    "        longitud_final_std = np.std(longitudes_finales)\n",
    "        \n",
    "        # Tiempo promedio\n",
    "        tiempo_mean = np.mean(resultados[metodo]['tiempos'])\n",
    "        tiempo_std = np.std(resultados[metodo]['tiempos'])\n",
    "        \n",
    "        # Episodios para converger (cuando la longitud media m√≥vil < 150)\n",
    "        convergencia_episodios = []\n",
    "        for run_longitudes in resultados[metodo]['longitudes']:\n",
    "            ventana = 100\n",
    "            medias_moviles = [np.mean(run_longitudes[i:i+ventana]) for i in range(len(run_longitudes)-ventana)]\n",
    "            try:\n",
    "                episodio_convergencia = next(i for i, media in enumerate(medias_moviles) if media < 150)\n",
    "                convergencia_episodios.append(episodio_convergencia)\n",
    "            except StopIteration:\n",
    "                convergencia_episodios.append(len(run_longitudes))\n",
    "        \n",
    "        convergencia_mean = np.mean(convergencia_episodios)\n",
    "        convergencia_std = np.std(convergencia_episodios)\n",
    "        \n",
    "        metricas.append({\n",
    "            'M√©todo': metodo,\n",
    "            'Recompensa Final': f\"{recompensa_final_mean:.2f} ¬± {recompensa_final_std:.2f}\",\n",
    "            'Longitud Final': f\"{longitud_final_mean:.1f} ¬± {longitud_final_std:.1f}\",\n",
    "            'Tiempo (s)': f\"{tiempo_mean:.1f} ¬± {tiempo_std:.1f}\",\n",
    "            'Episodios a Convergencia': f\"{convergencia_mean:.0f} ¬± {convergencia_std:.0f}\"\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metricas)\n",
    "    return df\n",
    "\n",
    "tabla_resumen = crear_tabla_resumen(resultados)\n",
    "print(\"üìä Tabla Resumen de Resultados:\")\n",
    "print(\"=\"*80)\n",
    "print(tabla_resumen.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbac19",
   "metadata": {},
   "source": [
    "# An√°lisis de Caracter√≠sticas\n",
    "\n",
    "Vamos a analizar las caracter√≠sticas de cada m√©todo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_caracteristicas():\n",
    "    \"\"\"Analiza las caracter√≠sticas de cada m√©todo de aproximaci√≥n.\"\"\"\n",
    "    \n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    \n",
    "    # Crear agentes con configuraciones t√≠picas\n",
    "    agente_fourier = AgenteSARSASemiGradienteFourier(env, fourier_order=3)\n",
    "    agente_tile = AgenteSARSATileCoding(env, bins=(8, 8), num_tilings=8)\n",
    "    agente_rbf = AgenteSARSARBF(env, num_centers=(5, 5), sigma=0.2)\n",
    "    \n",
    "    # Estado de ejemplo\n",
    "    state_ejemplo = np.array([-0.5, 0.0])  # posici√≥n media, velocidad cero\n",
    "    \n",
    "    # Obtener caracter√≠sticas\n",
    "    phi_fourier = agente_fourier._phi(state_ejemplo)\n",
    "    phi_tile = agente_tile.tile_coder.get_features(state_ejemplo)\n",
    "    phi_rbf = agente_rbf._phi(state_ejemplo)\n",
    "    \n",
    "    print(\"üîç An√°lisis de Caracter√≠sticas para estado [-0.5, 0.0]:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Fourier (orden {agente_fourier.fourier_order}):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_fourier)}\")\n",
    "    print(f\"  - Caracter√≠sticas activas: {np.sum(phi_fourier != 0)}\")\n",
    "    print(f\"  - Rango de valores: [{phi_fourier.min():.3f}, {phi_fourier.max():.3f}]\")\n",
    "    print(f\"  - Caracter√≠sticas top-5: {phi_fourier[:5]}\")\n",
    "    \n",
    "    print(f\"\\nTile Coding ({agente_tile.tile_coder.bins} bins, {agente_tile.tile_coder.num_tilings} tilings):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_tile)}\")\n",
    "    print(f\"  - Caracter√≠sticas activas: {np.sum(phi_tile != 0)}\")\n",
    "    print(f\"  - Caracter√≠sticas binarias: {np.all(np.isin(phi_tile, [0, 1]))}\")\n",
    "    print(f\"  - √çndices activos: {np.where(phi_tile == 1)[0]}\")\n",
    "    \n",
    "    print(f\"\\nRBF ({agente_rbf.rbf.num_centers} centros, œÉ={agente_rbf.rbf.sigma}):\")\n",
    "    print(f\"  - Dimensionalidad: {len(phi_rbf)}\")\n",
    "    print(f\"  - Caracter√≠sticas activas: {np.sum(phi_rbf > 0.01)}\")\n",
    "    print(f\"  - Rango de valores: [{phi_rbf.min():.3f}, {phi_rbf.max():.3f}]\")\n",
    "    print(f\"  - Caracter√≠sticas top-5: {phi_rbf[:5]}\")\n",
    "    \n",
    "    # Visualizar caracter√≠sticas\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Fourier\n",
    "    axes[0].bar(range(len(phi_fourier)), phi_fourier, alpha=0.7)\n",
    "    axes[0].set_title(f'Caracter√≠sticas Fourier (dim={len(phi_fourier)})')\n",
    "    axes[0].set_xlabel('√çndice de caracter√≠stica')\n",
    "    axes[0].set_ylabel('Valor')\n",
    "    \n",
    "    # Tile Coding\n",
    "    indices_activos = np.where(phi_tile == 1)[0]\n",
    "    if len(indices_activos) > 0:\n",
    "        axes[1].bar(indices_activos, phi_tile[indices_activos], alpha=0.7, color='orange')\n",
    "    axes[1].set_title(f'Caracter√≠sticas Tile Coding (dim={len(phi_tile)})')\n",
    "    axes[1].set_xlabel('√çndice de caracter√≠stica')\n",
    "    axes[1].set_ylabel('Valor')\n",
    "    \n",
    "    # RBF\n",
    "    axes[2].bar(range(len(phi_rbf)), phi_rbf, alpha=0.7, color='green')\n",
    "    axes[2].set_title(f'Caracter√≠sticas RBF (dim={len(phi_rbf)})')\n",
    "    axes[2].set_xlabel('√çndice de caracter√≠stica')\n",
    "    axes[2].set_ylabel('Valor')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "analizar_caracteristicas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a14eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
