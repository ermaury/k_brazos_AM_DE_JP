{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e05b34e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ermaury/k_brazos_AM_DE_JP/blob/main/P1/bandit_experiment_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45718ddbdacc17ac",
   "metadata": {
    "id": "45718ddbdacc17ac"
   },
   "source": [
    "# Estudio comparativo de algoritmos en un problema de k-armed bandit\n",
    "\n",
    "*Description:* El experimento compara el rendimiento de algoritmos Softmax y Gradiente de preferencias en un problema de k-armed bandit.\n",
    "Se generan gráficas de recompensas promedio para cada algoritmo.\n",
    "\n",
    "    Author: Luis Daniel Hernández Molinero\n",
    "    Email: ldaniel@um.es\n",
    "    Date: 2025/01/29\n",
    "\n",
    "This software is licensed under the GNU General Public License v3.0 (GPL-3.0),\n",
    "with the additional restriction that it may not be used for commercial purposes.\n",
    "\n",
    "For more details about GPL-3.0: https://www.gnu.org/licenses/gpl-3.0.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1697e197fa5a08",
   "metadata": {
    "id": "7c1697e197fa5a08"
   },
   "source": [
    "## Preparación del entorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b761138-de88-4ba2-acbf-70294b97f8ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:27:42.181467Z",
     "iopub.status.busy": "2025-06-19T22:27:42.181290Z",
     "iopub.status.idle": "2025-06-19T22:27:42.192840Z",
     "shell.execute_reply": "2025-06-19T22:27:42.191993Z",
     "shell.execute_reply.started": "2025-06-19T22:27:42.181450Z"
    },
    "id": "8b761138-de88-4ba2-acbf-70294b97f8ca",
    "outputId": "a0849bd8-fce4-427a-8d89-4bbe4fa5f5f0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Comprobar si estamos en Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    EN_COLAB = True\n",
    "except ImportError:\n",
    "    EN_COLAB = False\n",
    "\n",
    "if EN_COLAB:\n",
    "    # Descargar el repositorio\n",
    "    !git clone https://github.com/ermaury/k_brazos_AM_DE_JP.git\n",
    "\n",
    "    # Cambiar al directorio de trabajo donde está P1\n",
    "    repo_path = '/content/k_brazos_AM_DE_JP/P1'\n",
    "    os.chdir(repo_path)\n",
    "\n",
    "    # Confirmar cambio de directorio y añadir al path\n",
    "    print(\"Directorio actual:\", os.getcwd())\n",
    "    sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4582eec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:16:53.845102Z",
     "start_time": "2025-01-29T15:16:53.842529Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:27:42.194953Z",
     "iopub.status.busy": "2025-06-19T22:27:42.194350Z",
     "iopub.status.idle": "2025-06-19T22:27:43.246083Z",
     "shell.execute_reply": "2025-06-19T22:27:43.245332Z",
     "shell.execute_reply.started": "2025-06-19T22:27:42.194931Z"
    },
    "id": "4582eec6",
    "outputId": "425ded16-3421-4998-d82d-8f06db02a841"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from algorithms import Algorithm, GradientBandit, SoftmaxBandit\n",
    "from arms import ArmNormal, ArmBernouilli, ArmBinomial, Bandit\n",
    "from plotting import plot_average_rewards, plot_optimal_selections, plot_regret, plot_arm_statistics\n",
    "\n",
    "\n",
    "semilla = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67de1a19a3698f",
   "metadata": {
    "id": "4e67de1a19a3698f"
   },
   "source": [
    "## Experimento\n",
    "\n",
    "Cada algoritmo se ejecuta en un problema de k-armed bandit durante un número de pasos de tiempo y ejecuciones determinado.\n",
    "Se comparan los resultados de los algoritmos en términos de recompensa promedio.\n",
    "\n",
    "Por ejemplo. Dado un bandido de k-brazos, se ejecutan dos algoritmos epsilon-greedy con diferentes valores de epsilon. Se estudia la evolución de cada política  en un número de pasos, por ejemplo, mil pasos. Entonces se repite el experimento un número de veces, por ejemplo, 500 veces. Es decir, se ejecutan 500 veces la evolución de cada algoritmo en 1000 pasos. Para cada paso calculamos el promedio de las recoponensas obtenidas en esas 500 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aca77e1-b3eb-40f9-9b5c-c82f197b0c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:27:43.247210Z",
     "iopub.status.busy": "2025-06-19T22:27:43.246942Z",
     "iopub.status.idle": "2025-06-19T22:27:43.255001Z",
     "shell.execute_reply": "2025-06-19T22:27:43.254156Z",
     "shell.execute_reply.started": "2025-06-19T22:27:43.247192Z"
    },
    "id": "4aca77e1-b3eb-40f9-9b5c-c82f197b0c1a"
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algorithms: List, steps: int, runs: int):\n",
    "    \"\"\"\n",
    "    Ejecuta experimentos con múltiples algoritmos sobre un bandit dado.\n",
    "\n",
    "    :param bandit: Instancia de la clase Bandit.\n",
    "    :param algorithms: Lista de instancias de algoritmos a evaluar.\n",
    "    :param steps: Número de pasos de tiempo a simular.\n",
    "    :param runs: Número de ejecuciones para promediar los resultados.\n",
    "    \n",
    "    :return: rewards, optimal_selections, regret_accumulated, arm_stats - \n",
    "             Matrices con recompensas, selecciones óptimas, regret acumulado y estadísticas de brazos.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(semilla)  # Asegurar reproducibilidad\n",
    "    \n",
    "    optimal_arm = bandit.optimal_arm  # Índice del brazo óptimo\n",
    "    optimal_reward = bandit.expected_rewards[optimal_arm]  # Recompensa esperada del brazo óptimo\n",
    "    num_arms = len(bandit.arms)  # Número total de brazos\n",
    "\n",
    "    rewards = np.zeros((len(algorithms), steps))  # Matriz para recompensas promedio\n",
    "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para selecciones óptimas\n",
    "    regrets = np.zeros((len(algorithms), steps))  # Matriz para regret acumulado\n",
    "\n",
    "    # Inicializar estadísticas de cada brazo por algoritmo\n",
    "    arm_stats = [\n",
    "        {\n",
    "            'total_rewards': np.zeros(num_arms),  # Suma de recompensas por brazo\n",
    "            'selections': np.zeros(num_arms),    # Veces que se seleccionó cada brazo\n",
    "            'optimal_arm': optimal_arm,          # Guardar cuál es el brazo óptimo\n",
    "        }\n",
    "        for _ in range(len(algorithms))\n",
    "    ]\n",
    "\n",
    "    for run in range(runs):\n",
    "        current_bandit = Bandit(arms=bandit.arms)  # Nueva instancia del bandit\n",
    "\n",
    "        # Reiniciar los algoritmos\n",
    "        for algo in algorithms:\n",
    "            algo.reset()\n",
    "\n",
    "        for step in range(steps):\n",
    "            for idx, algo in enumerate(algorithms):\n",
    "                chosen_arm = algo.select_arm()  # Seleccionar un brazo\n",
    "                reward = current_bandit.pull_arm(chosen_arm)  # Obtener recompensa\n",
    "                algo.update(chosen_arm, reward)  # Actualizar valores estimados\n",
    "\n",
    "                rewards[idx, step] += reward  # Acumular recompensa\n",
    "\n",
    "                # Si el brazo elegido es el óptimo, incrementar el contador\n",
    "                if chosen_arm == optimal_arm:\n",
    "                    optimal_selections[idx, step] += 1\n",
    "\n",
    "                # Cálculo del regret instantáneo\n",
    "                regret = optimal_reward - reward\n",
    "                regrets[idx, step] += regret\n",
    "\n",
    "                # Guardar estadísticas del brazo seleccionado\n",
    "                arm_stats[idx]['total_rewards'][chosen_arm] += reward\n",
    "                arm_stats[idx]['selections'][chosen_arm] += 1\n",
    "\n",
    "    # Promediar los valores sobre el número de ejecuciones\n",
    "    rewards /= runs\n",
    "    optimal_selections = (optimal_selections / runs) * 100  # Convertir a porcentaje\n",
    "    regrets /= runs  # Promediar regret acumulado\n",
    "\n",
    "    # Calcular promedio de recompensas por brazo\n",
    "    for idx in range(len(algorithms)):\n",
    "        # Evitar divisiones por cero (si un brazo no fue seleccionado)\n",
    "        mask = arm_stats[idx]['selections'] > 0\n",
    "        arm_stats[idx]['avg_rewards'] = np.zeros(num_arms)\n",
    "        arm_stats[idx]['avg_rewards'][mask] = arm_stats[idx]['total_rewards'][mask] / arm_stats[idx]['selections'][mask]\n",
    "        \n",
    "    return rewards, optimal_selections, regrets, arm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5453755a2b2a8",
   "metadata": {
    "id": "3fb5453755a2b2a8"
   },
   "source": [
    "## Ejecución del experimento\n",
    "\n",
    "Se realiza el experimento usando 10 brazos, cada uno de acuerdo a una distribución gaussina con desviación 1. Se realizan 500 ejecuciones de 1000 pasos cada una. Se contrastan 3 algoritmos de ascenso del gradiente para valores alpha: 0.0, 0.01, y 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bf5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:02.644124Z",
     "start_time": "2025-01-29T15:16:59.172459Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:27:43.256002Z",
     "iopub.status.busy": "2025-06-19T22:27:43.255795Z"
    },
    "id": "157bf5cc",
    "outputId": "f96fa95a-1e38-4b4c-a71f-24eee942f34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Bandit with arm type ArmNormal\n",
      "Optimal arm: 8 with expected reward=8.73\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del experimento\n",
    "np.random.seed(semilla)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "k = 10  # Número de brazos\n",
    "steps = 1000  # Número de pasos que se ejecutarán cada algoritmo\n",
    "runs = 500  # Número de ejecuciones\n",
    "\n",
    "arms = [ArmNormal, ArmBernouilli, ArmBinomial]\n",
    "\n",
    "# Creación del bandit\n",
    "bandits = [Bandit(arms=arm.generate_arms(k)) for arm in arms]\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for bandit in bandits:\n",
    "    print(f\"Executing Bandit with arm type {bandit.get_type_arms()}\")\n",
    "    \n",
    "    optimal_arm = bandit.optimal_arm\n",
    "    print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
    "    \n",
    "    # Definir los algoritmos a comparar. En este caso son 3 algoritmos SoftmaxBandit con diferentes valores de parametro tau y 3 algoritmos GradientBandit con distintos parametros alpha.\n",
    "    algorithms = [SoftmaxBandit(k=k, tau=0.01), SoftmaxBandit(k=k, tau=0.5), SoftmaxBandit(k=k, tau=1.0), GradientBandit(k=k, alpha=0.01), GradientBandit(k=k, alpha=0.05), GradientBandit(k=k, alpha=0.1)]\n",
    "    \n",
    "    # Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones óptimas\n",
    "    rewards, optimal_selections, regret_accumulated, arm_stats = run_experiment(bandit, algorithms, steps=1000, runs=500)\n",
    "\n",
    "    results[bandit.get_type_arms()] = (rewards, optimal_selections, regret_accumulated, arm_stats)\n",
    "\n",
    "\n",
    "    # Calcular la recompensa media total para cada algoritmo\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "\n",
    "    print(\"\\nRecompensas medias por algoritmo:\")\n",
    "    for algo, mean_reward in zip(algorithms, mean_rewards):\n",
    "        print(f\"{algo.get_algorithm_label()}: {mean_reward:.3f}\")\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebb5fb-8f08-485b-9043-a6b7960cd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_normal, optimal_selections_normal, regret_accumulated_normal, arm_stats_normal = results['ArmNormal']\n",
    "rewards_bernouilli, optimal_selections_bernouilli, regret_accumulated_bernouilli, arm_stats_bernouilli = results['ArmBernouilli']\n",
    "rewards_binomial, optimal_selections_binomial, regret_accumulated_binomial, arm_stats_binomial = results['ArmBinomial']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c9612c3b14efb",
   "metadata": {
    "id": "573c9612c3b14efb"
   },
   "source": [
    "## Visualización de los resultados (Brazo Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd68ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:07.750227Z",
     "start_time": "2025-01-29T15:17:07.603719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d3fd68ee",
    "outputId": "8430e8c8-7a36-453b-b950-3b53cf215d82"
   },
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_normal, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_normal, algorithms)\n",
    "plot_regret(steps, regret_accumulated_normal, algorithms)\n",
    "plot_arm_statistics(arm_stats_normal, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1a17e-61ad-46b0-a385-21e4f75d9c9f",
   "metadata": {},
   "source": [
    "\n",
    "### Comparación de Recompensa Promedio\n",
    "\n",
    "- **SoftmaxBandit con τ = 0.01** muestra un rendimiento deficiente. Apenas explora y converge rápidamente a una estrategia subóptima, con una recompensa promedio estable alrededor de 6.8.\n",
    "- **SoftmaxBandit con τ = 0.5 y τ = 1.0** exploran más y logran converger a una recompensa promedio cercana al óptimo (~8.4), mostrando un buen equilibrio entre exploración y explotación.\n",
    "- **GradientBandit** con distintos valores de α:\n",
    "  - α = 0.01: presenta una evolución más lenta pero constante, convergiendo con precisión al óptimo.\n",
    "  - α = 0.05 y 0.1: alcanzan buena recompensa promedio más rápidamente, aunque con algo más de variabilidad al inicio.\n",
    "\n",
    "### Comparación de Selección del Brazo Óptimo\n",
    "\n",
    "- **SoftmaxBandit con ``τ = 0.01``** selecciona el brazo óptimo con muy poca frecuencia (~20%), evidenciando falta de exploración.\n",
    "- Con **``τ = 0.5`` y ``τ = 1.0``**, el porcentaje de selección óptima supera el 80% tras 1000 pasos.\n",
    "- **GradientBandit** con ``α = 0.05`` y ``0.1`` también superan el 80% (``α = 0.05`` incluso supera el 90%), mientras que ``α = 0.01`` tarda más en estabilizarse pero lo consigue.\n",
    "\n",
    "### Comparación del Regret\n",
    "\n",
    "- El **regret más bajo** lo alcanzan `GradientBandit (α = 0.1)` y `SoftmaxBandit (α = 0.05)`, lo que indica una convergencia rápida y sostenida hacia el brazo óptimo.\n",
    "- `SoftmaxBandit (τ = 0.01)` mantiene un regret constante debido a la falta de aprendizaje efectivo.\n",
    "- El resto de configuraciones muestran una rápida disminución del regret, con valores cercanos a cero al final del horizonte temporal.\n",
    "\n",
    "### Estadísticas de los Brazos\n",
    "\n",
    "- Las gráficas de estadísticas muestran que:\n",
    "  - Los algoritmos `Softmax τ = 0.5` y `1.0`, `GradientBandit α = 0.05 y 0.1` asignan la mayoría de las selecciones al brazo óptimo.\n",
    "  - `SoftmaxBandit (τ = 0.01)` distribuye las selecciones de forma más dispersa y subóptima, sin concentrarse en el mejor brazo.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Los métodos de ascenso del gradiente son eficaces cuando se configuran adecuadamente. Tanto `SoftmaxBandit` como `GradientBandit` pueden superar a Epsilon-Greedy en tareas con recompensas continuas (como la distribución normal), siempre que la temperatura (τ) o la tasa de aprendizaje (α) se seleccionen correctamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409706e4-4c84-4e96-9b40-46c5f7cf7c8f",
   "metadata": {},
   "source": [
    "## Visualización de los resultados (Brazo Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da960e5-49bf-4340-8e73-c5032bbd2cf7",
   "metadata": {
    "id": "4207366e56a23449"
   },
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_bernouilli, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_bernouilli, algorithms)\n",
    "plot_regret(steps, regret_accumulated_bernouilli, algorithms)\n",
    "plot_arm_statistics(arm_stats_bernouilli, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2891db-1c70-4a5e-880f-ed50c1275487",
   "metadata": {},
   "source": [
    "### Comparación de Recompensa Promedio\n",
    "\n",
    "- **SoftmaxBandit con τ = 0.01** comienza con buena recompensa promedio (≈0.7) pero se estabiliza rápidamente sin mejorar, indicando poca exploración.\n",
    "- **SoftmaxBandit con τ = 0.5 y τ = 1.0** exploran mejor, alcanzando recompensas promedio por encima de 0.8, con τ = 0.5 ligeramente superior.\n",
    "- **GradientBandit**:\n",
    "  - **α = 0.01** tiene un rendimiento bajo, avanzando lentamente hacia recompensas aceptables (~0.5).\n",
    "  - **α = 0.05** muestra mejoras constantes y sostenidas, alcanzando valores cercanos a los de Softmax con τ = 1.0.\n",
    "  - **α = 0.1** mejora más y más rápido.\n",
    "\n",
    "### Comparación de Selección del Brazo Óptimo\n",
    "\n",
    "- **SoftmaxBandit (``τ = 0.01``)** se estanca en ~30% de selección óptima.\n",
    "- **SoftmaxBandit (``τ = 0.5`` y ``τ = 1.0``)** alcanzan entre el 50% y 55% de selección del brazo óptimo.\n",
    "- **GradientBandit con ``α = 0.1``** alcanza porcentajes similares, aunque el progreso es más lento.\n",
    "- **GradientBandit (``α = 0.01``)** tiene dificultades claras para identificar el brazo óptimo en este entorno.\n",
    "\n",
    "### Comparación del Regret\n",
    "\n",
    "- **SoftmaxBandit (τ = 0.5)** tiene el regret más bajo y consistente.\n",
    "- Le siguen **SoftmaxBandit (τ = 1.0)** y **GradientBandit (α = 0.1)**, que también alcanzan buen rendimiento en el tiempo.\n",
    "- **SoftmaxBandit (τ = 0.01)** y **GradientBandit (α = 0.01)** acumulan más regret por su falta de exploración o lentitud en el aprendizaje.\n",
    "\n",
    "### Estadísticas de los Brazos\n",
    "\n",
    "- **SoftmaxBandit con τ = 0.5** asigna la mayoría de selecciones al brazo óptimo, seguido por **τ = 1.0** y **GradientBandit (α = 0.1)**.\n",
    "- Los algoritmos con parámetros mal ajustados (como **τ = 0.01** y **α = 0.01**) reparten más las selecciones entre brazos subóptimos.\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "En entornos Bernoulli, los métodos de ascenso del gradiente pueden competir con Epsilon-Greedy si están bien configurados. El parámetro τ en Softmax y α en GradientBandit deben equilibrar exploración y explotación: valores intermedios como **τ = 0.5** o **α = 0.05** muestran el mejor compromiso entre rendimiento, estabilidad y baja variabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6838838-e5a4-483d-89e5-0855f72bda7d",
   "metadata": {},
   "source": [
    "## Visualización de los resultados (Brazo distribución Binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e04ca-e391-4374-95cd-b4ed1e20aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_binomial, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_binomial, algorithms)\n",
    "plot_regret(steps, regret_accumulated_binomial, algorithms)\n",
    "plot_arm_statistics(arm_stats_binomial, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5998b5b-ddd9-4132-be28-8662f7059af9",
   "metadata": {},
   "source": [
    "### Recompensa Promedio\n",
    "\n",
    "La recompensa promedio permite evaluar la eficiencia global del algoritmo a lo largo del tiempo:\n",
    "\n",
    "- **SoftmaxBandit con `tau=0.01`** muestra una rápida estabilización, pero se queda lejos del óptimo. La baja temperatura produce una política demasiado conservadora (cercana a la explotación).\n",
    "- **SoftmaxBandit con `tau=0.5` y `tau=1.0`** presentan mejoras significativas, alcanzando valores cercanos al óptimo con una mayor exploración, especialmente `tau=1.0`, que muestra una convergencia más estable.\n",
    "- **GradientBandit con baseline (`alpha=0.05` y `alpha=0.1`)** logran las mejores recompensas promedio, convergiendo rápidamente y con poca oscilación. `alpha=0.1` obtiene una ventaja en la rapidez inicial.\n",
    "- **GradientBandit con `alpha=0.01`** mejora progresivamente, pero a un ritmo más lento.\n",
    "\n",
    "### Porcentaje de Selección del Brazo Óptimo\n",
    "\n",
    "Este indicador mide con qué frecuencia el algoritmo identifica y selecciona el brazo óptimo:\n",
    "\n",
    "- **SoftmaxBandit con (`tau=0.5` y `tau=1.0`)** alcanzan el 80% y 90% respectivamente de selección óptima tras varias centenas de pasos.\n",
    "- **GradientBandit con baseline y `alpha=0.01` y `alpha=0.05`** alcanzan el casi el 100% de selección del brazo óptimo, aunque `alpha=0.05` lo alcanza mucho antes.\n",
    "- **SoftmaxBandit con (`tau=0.01` y `tau=0.5`)** se quedan notablemente por debajo (especialmente `tau=0.01`), con dificultades para explorar suficientemente.\n",
    "\n",
    "### Regret\n",
    "\n",
    "El regret mide la pérdida acumulada por no elegir siempre el brazo óptimo:\n",
    "\n",
    "- **Softmax con `tau=0.01`** acumula el mayor regret, debido a su baja exploración.\n",
    "- **GradientBandit (`alpha=0.05` y `alpha=0.1`)** presentan los menores valores de regret a lo largo de los 1000 pasos.\n",
    "- **Softmax (`tau=0.5` y `tau=1.0`)** logran resultados competitivos, aunque ligeramente por debajo de GradientBandit (especialmente en el caso de ``tau = 0.5``).\n",
    "- **GradientBandit con `alpha=0.01`** mejora lentamente, y por ello su regret es algo más elevado.\n",
    "\n",
    "### Estadísticas de Brazos\n",
    "\n",
    "- Los histogramas confirman las observaciones previas:\n",
    "  - **Softmax con `tau=0.01`** concentra demasiadas selecciones en brazos subóptimos.\n",
    "  - **Softmax con `tau=1.0`** y **GradientBandit (`alpha=0.05`, `alpha=0.1`)** muestran una clara preferencia por el brazo óptimo.\n",
    "  - **GradientBandit con `alpha=0.01`** distribuye las selecciones más uniformemente, reflejo de su lento aprendizaje.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "En el entorno con distribución **binomial**, el algoritmo **GradientBandit con baseline y tasas de aprendizaje entre `0.05` y `0.1`** resulta ser el más eficiente y consistente. El método **Softmax** también obtiene buenos resultados con valores intermedios de temperatura (`tau=0.5` o `1.0`), pero muestra mayor sensibilidad a la elección del parámetro. En cambio, los valores demasiado bajos de `tau` o `alpha` provocan una exploración insuficiente que perjudica el rendimiento global.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675759a6-2e55-4c77-aa4e-ca1e1c34a478",
   "metadata": {},
   "source": [
    "## Conclusión general\n",
    "\n",
    "Se observó que el algoritmo Gradient Bandit, al operar directamente sobre las preferencias $H(a)$ y ajustar las probabilidades de forma implícita vía softmax, es menos sensible al ruido estocástico que las actualizaciones directas de estimación de recompensa utilizadas en Softmax.\r\n",
    "\r\n",
    "Los gráficos de porcentaje de selección óptima evidencian que Gradient Bandit tiende a superar progresivamente a Softmax en las tres configuraciones de distribución. Sin embargo, Softmax sigue ofreciendo una alternativa robusta en escenarios no estacionarios debido a su exploración continua.\r\n",
    "\r\n",
    "En términos de regret acumulado, Gradient Bandit demostró el crecimiento más lento del rechazo, confirmando su mayor eficiencia en converger hacia el mejor brazo en un número reducido de pasos.\r\n",
    "\r\n",
    "Estas observaciones refuerzan el potencial de los métodos de gradiente directo cuando se dispone de suficiente exploración inicial y una adecuada elección de tasa de aprendizaje.\r\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
