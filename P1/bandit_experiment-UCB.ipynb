{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e05b34e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ermaury/k_brazos_AM_DE_JP/blob/main/P1/bandit_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45718ddbdacc17ac",
   "metadata": {
    "id": "45718ddbdacc17ac"
   },
   "source": [
    "# Estudio comparativo de algoritmos en un problema de k-armed bandit\n",
    "\n",
    "*Description:* El experimento compara el rendimiento de algoritmos UCB (Upper Confidence Bound) en un problema de k-armed bandit.\n",
    "Se generan gráficas de recompensas promedio para cada algoritmo.\n",
    "\n",
    "    Author: Luis Daniel Hernández Molinero\n",
    "    Email: ldaniel@um.es\n",
    "    Date: 2025/01/29\n",
    "\n",
    "This software is licensed under the GNU General Public License v3.0 (GPL-3.0),\n",
    "with the additional restriction that it may not be used for commercial purposes.\n",
    "\n",
    "For more details about GPL-3.0: https://www.gnu.org/licenses/gpl-3.0.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1697e197fa5a08",
   "metadata": {
    "id": "7c1697e197fa5a08"
   },
   "source": [
    "## Preparación del entorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b761138-de88-4ba2-acbf-70294b97f8ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:29:20.298141Z",
     "iopub.status.busy": "2025-06-19T22:29:20.297790Z",
     "iopub.status.idle": "2025-06-19T22:29:20.308130Z",
     "shell.execute_reply": "2025-06-19T22:29:20.307292Z",
     "shell.execute_reply.started": "2025-06-19T22:29:20.298111Z"
    },
    "id": "X46D2jGX88vn",
    "outputId": "10d3a0f1-95b3-446f-fe51-25174d53225f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Comprobar si estamos en Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    EN_COLAB = True\n",
    "except ImportError:\n",
    "    EN_COLAB = False\n",
    "\n",
    "if EN_COLAB:\n",
    "    # Descargar el repositorio\n",
    "    !git clone https://github.com/ermaury/k_brazos_AM_DE_JP.git\n",
    "\n",
    "    # Cambiar al directorio de trabajo donde está P1\n",
    "    repo_path = '/content/k_brazos_AM_DE_JP/P1'\n",
    "    os.chdir(repo_path)\n",
    "\n",
    "    # Confirmar cambio de directorio y añadir al path\n",
    "    print(\"Directorio actual:\", os.getcwd())\n",
    "    sys.path.append(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4582eec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:16:53.845102Z",
     "start_time": "2025-01-29T15:16:53.842529Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:29:20.310118Z",
     "iopub.status.busy": "2025-06-19T22:29:20.309738Z",
     "iopub.status.idle": "2025-06-19T22:29:21.797699Z",
     "shell.execute_reply": "2025-06-19T22:29:21.796972Z",
     "shell.execute_reply.started": "2025-06-19T22:29:20.310098Z"
    },
    "id": "4582eec6",
    "outputId": "253fdbf8-7d44-4f9f-cc28-1a7d71db633a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "from algorithms import Algorithm, UCB1, UCB2\n",
    "from arms import ArmNormal, ArmBernouilli, ArmBinomial, ArmBeta, Bandit\n",
    "from plotting import plot_average_rewards, plot_optimal_selections, plot_regret, plot_arm_statistics\n",
    "\n",
    "\n",
    "semilla = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67de1a19a3698f",
   "metadata": {
    "id": "4e67de1a19a3698f"
   },
   "source": [
    "## Experimento\n",
    "\n",
    "Cada algoritmo se ejecuta en un problema de k-armed bandit durante un número de pasos de tiempo y ejecuciones determinado.\n",
    "Se comparan los resultados de los algoritmos en términos de recompensa promedio.\n",
    "\n",
    "Por ejemplo. Dado un bandido de k-brazos, se ejecutan dos algoritmos epsilon-greedy con diferentes valores de epsilon. Se estudia la evolución de cada política  en un número de pasos, por ejemplo, mil pasos. Entonces se repite el experimento un número de veces, por ejemplo, 500 veces. Es decir, se ejecutan 500 veces la evolución de cada algoritmo en 1000 pasos. Para cada paso calculamos el promedio de las recoponensas obtenidas en esas 500 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aca77e1-b3eb-40f9-9b5c-c82f197b0c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T22:29:21.798839Z",
     "iopub.status.busy": "2025-06-19T22:29:21.798524Z",
     "iopub.status.idle": "2025-06-19T22:29:21.807258Z",
     "shell.execute_reply": "2025-06-19T22:29:21.806238Z",
     "shell.execute_reply.started": "2025-06-19T22:29:21.798821Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(bandit, algorithms: List, steps: int, runs: int):\n",
    "    \"\"\"\n",
    "    Ejecuta experimentos con múltiples algoritmos sobre un bandit dado.\n",
    "\n",
    "    :param bandit: Instancia de la clase Bandit.\n",
    "    :param algorithms: Lista de instancias de algoritmos a evaluar.\n",
    "    :param steps: Número de pasos de tiempo a simular.\n",
    "    :param runs: Número de ejecuciones para promediar los resultados.\n",
    "    \n",
    "    :return: rewards, optimal_selections, regret_accumulated, arm_stats - \n",
    "             Matrices con recompensas, selecciones óptimas, regret acumulado y estadísticas de brazos.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(semilla)  # Asegurar reproducibilidad\n",
    "    \n",
    "    optimal_arm = bandit.optimal_arm  # Índice del brazo óptimo\n",
    "    optimal_reward = bandit.expected_rewards[optimal_arm]  # Recompensa esperada del brazo óptimo\n",
    "    num_arms = len(bandit.arms)  # Número total de brazos\n",
    "\n",
    "    rewards = np.zeros((len(algorithms), steps))  # Matriz para recompensas promedio\n",
    "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para selecciones óptimas\n",
    "    regrets = np.zeros((len(algorithms), steps))  # Matriz para regret acumulado\n",
    "\n",
    "    # Inicializar estadísticas de cada brazo por algoritmo\n",
    "    arm_stats = [\n",
    "        {\n",
    "            'total_rewards': np.zeros(num_arms),  # Suma de recompensas por brazo\n",
    "            'selections': np.zeros(num_arms),    # Veces que se seleccionó cada brazo\n",
    "            'optimal_arm': optimal_arm,          # Guardar cuál es el brazo óptimo\n",
    "        }\n",
    "        for _ in range(len(algorithms))\n",
    "    ]\n",
    "\n",
    "    for run in range(runs):\n",
    "        current_bandit = Bandit(arms=bandit.arms)  # Nueva instancia del bandit\n",
    "\n",
    "        # Reiniciar los algoritmos\n",
    "        for algo in algorithms:\n",
    "            algo.reset()\n",
    "\n",
    "        for step in range(steps):\n",
    "            for idx, algo in enumerate(algorithms):\n",
    "                chosen_arm = algo.select_arm()  # Seleccionar un brazo\n",
    "                reward = current_bandit.pull_arm(chosen_arm)  # Obtener recompensa\n",
    "                algo.update(chosen_arm, reward)  # Actualizar valores estimados\n",
    "\n",
    "                rewards[idx, step] += reward  # Acumular recompensa\n",
    "\n",
    "                # Si el brazo elegido es el óptimo, incrementar el contador\n",
    "                if chosen_arm == optimal_arm:\n",
    "                    optimal_selections[idx, step] += 1\n",
    "\n",
    "                # Cálculo del regret instantáneo\n",
    "                regret = optimal_reward - reward\n",
    "                regrets[idx, step] += regret\n",
    "\n",
    "                # Guardar estadísticas del brazo seleccionado\n",
    "                arm_stats[idx]['total_rewards'][chosen_arm] += reward\n",
    "                arm_stats[idx]['selections'][chosen_arm] += 1\n",
    "\n",
    "    # Promediar los valores sobre el número de ejecuciones\n",
    "    rewards /= runs\n",
    "    optimal_selections = (optimal_selections / runs) * 100  # Convertir a porcentaje\n",
    "    regrets /= runs  # Promediar regret acumulado\n",
    "\n",
    "    # Calcular promedio de recompensas por brazo\n",
    "    for idx in range(len(algorithms)):\n",
    "        # Evitar divisiones por cero (si un brazo no fue seleccionado)\n",
    "        mask = arm_stats[idx]['selections'] > 0\n",
    "        arm_stats[idx]['avg_rewards'] = np.zeros(num_arms)\n",
    "        arm_stats[idx]['avg_rewards'][mask] = arm_stats[idx]['total_rewards'][mask] / arm_stats[idx]['selections'][mask]\n",
    "        \n",
    "    return rewards, optimal_selections, regrets, arm_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5453755a2b2a8",
   "metadata": {
    "id": "3fb5453755a2b2a8"
   },
   "source": [
    "Ejecución del experimento\n",
    "\n",
    "Se realiza el experimento usando 10 brazos, cada uno de acuerdo a una distribución gaussina con desviación 1. Se realizan 500 ejecuciones de 1000 pasos cada una. Se contrastan los dos algoritmos UCB para valores alpha = 0.5, 0.1, y 0.01 en el caso de UCB2 y c = 2, 1, 0.5 en el caso de UCB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157bf5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:02.644124Z",
     "start_time": "2025-01-29T15:16:59.172459Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-19T22:29:21.808663Z",
     "iopub.status.busy": "2025-06-19T22:29:21.808256Z"
    },
    "id": "157bf5cc",
    "outputId": "5c742dc9-f0ee-4d7e-f2e1-794b0883f0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Bandit with arm type ArmNormal\n",
      "Optimal arm: 8 with expected reward=8.73\n"
     ]
    }
   ],
   "source": [
    "# Parámetros del experimento\n",
    "np.random.seed(semilla)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "k = 10  # Número de brazos\n",
    "steps = 1000  # Número de pasos que se ejecutarán cada algoritmo\n",
    "runs = 500  # Número de ejecuciones\n",
    "\n",
    "arms = [ArmNormal, ArmBernouilli, ArmBinomial]\n",
    "\n",
    "# Creación del bandit\n",
    "bandits = [Bandit(arms=arm.generate_arms(k)) for arm in arms]\n",
    "\n",
    "results = dict()\n",
    "\n",
    "for bandit in bandits:\n",
    "    print(f\"Executing Bandit with arm type {bandit.get_type_arms()}\")\n",
    "    \n",
    "    optimal_arm = bandit.optimal_arm\n",
    "    print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
    "    \n",
    "    # Definir los algoritmos a comparar. En este caso son 3 algoritmos UCB1 con diferentes valores de parametro c y 3 algoritmos UCB2 con distintos parametros alpha.\n",
    "    algorithms = [UCB1(k=k, c=0.01), UCB1(k=k, c=1.0), UCB1(k=k, c=3.0), UCB2(k=k, alpha=0.01), UCB2(k=k, alpha=0.3), UCB2(k=k, alpha=0.5)]\n",
    "    \n",
    "    # Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones óptimas\n",
    "    rewards, optimal_selections, regret_accumulated, arm_stats = run_experiment(bandit, algorithms, steps=1000, runs=500)\n",
    "\n",
    "    results[bandit.get_type_arms()] = (rewards, optimal_selections, regret_accumulated, arm_stats)\n",
    "\n",
    "    # Calcular la recompensa media total para cada algoritmo\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "\n",
    "    print(\"\\nRecompensas medias por algoritmo:\")\n",
    "    for algo, mean_reward in zip(algorithms, mean_rewards):\n",
    "        print(f\"{algo.get_algorithm_label()}: {mean_reward:.3f}\")\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed10896-c008-44c2-a5a6-9556dbbd4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_normal, optimal_selections_normal, regret_accumulated_normal, arm_stats_normal = results['ArmNormal']\n",
    "rewards_bernouilli, optimal_selections_bernouilli, regret_accumulated_bernouilli, arm_stats_bernouilli = results['ArmBernouilli']\n",
    "rewards_binomial, optimal_selections_binomial, regret_accumulated_binomial, arm_stats_binomial = results['ArmBinomial']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c9612c3b14efb",
   "metadata": {
    "id": "573c9612c3b14efb"
   },
   "source": [
    "## Visualización de los resultados (Brazo Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb392c-df16-4462-b770-ea4e1e0ddc14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:17:07.750227Z",
     "start_time": "2025-01-29T15:17:07.603719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d3fd68ee",
    "outputId": "4e2f13c4-9d38-48bc-afbe-940834eb0b9f"
   },
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_normal, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_normal, algorithms)\n",
    "plot_regret(steps, regret_accumulated_normal, algorithms)\n",
    "plot_arm_statistics(arm_stats_normal, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c711748-e3bb-4ced-a957-90c028346db4",
   "metadata": {},
   "source": [
    "### Recompensa Promedio vs Pasos de Tiempo\n",
    "\n",
    "- Todos los algoritmos convergen a una recompensa promedio alta (≈ 8.5-8.7).\n",
    "- **UCB1 con `c = 3.0`** tiene un inicio más lento (más exploración), pero alcanza buen rendimiento.\n",
    "- **UCB2 (todos los valores de `alpha`)** presentan una subida rápida y estable.\n",
    "\n",
    "---\n",
    "\n",
    "### Porcentaje de Selección del Brazo Óptimo\n",
    "\n",
    "- **Todos los algoritmos superan el 95%** de selección del óptimo, **excepto UCB1 con `c = 0.01`**, que se queda cerca del **90%**.\n",
    "- **UCB1 con `c = 3.0`** muestra una convergencia más lenta, pero finalmente se acerca a los mejores valores.\n",
    "- **UCB1 con `c = 0.01`** muestra una convergencia más temprana, pero su rendimiento final es **ligeramente inferior**, debido a falta de exploración suficiente.\n",
    "- **UCB2 (`alpha = 0.01`, `0.3`, `0.5`)** converge muy rápidamente y de forma estable hacia la selección del óptimo.\n",
    "\n",
    "Este gráfico refleja que **una exploración demasiado baja puede ser perjudicial**, como ocurre con `c = 0.01`.\n",
    "\n",
    "---\n",
    "\n",
    "### Regret Acumulado vs Pasos de Tiempo\n",
    "\n",
    "- Todos los algoritmos reducen el regret con el tiempo.\n",
    "- **UCB1 con `c = 3.0`** acumula más regret al inicio, pero lo reduce progresivamente.\n",
    "- **UCB2 (todos)** tienen un regret bajo desde los primeros pasos.\n",
    "\n",
    "---\n",
    "\n",
    "### Estadísticas de Selección y Recompensa por Brazo\n",
    "\n",
    "- **UCB1 con `c = 1.0`** y **UCB2 con `alpha = 0.3` y `0.5`** obtienen los mejores resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "En entornos con **recompensas normales**, los algoritmos UCB muestran muy buen rendimiento general, pero:\n",
    "\n",
    "- **Explorar muy poco (`c = 0.01`)** puede ser contraproducente, ya que impide corregir malas decisiones iniciales.\n",
    "- **Explorar demasiado (`c = 3.0` o `alpha = 0.5`)** puede ralentizar la explotación, pero garantiza convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb78236-c0e1-4aae-9c91-b6aa68005f24",
   "metadata": {},
   "source": [
    "## Visualización de los resultados (Distribución Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2181be-982f-4ae2-920e-53e35f2d3665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_bernouilli, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_bernouilli, algorithms)\n",
    "plot_regret(steps, regret_accumulated_bernouilli, algorithms)\n",
    "plot_arm_statistics(arm_stats_bernouilli, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44aefad-f5e3-497a-a80e-a5c604b596ac",
   "metadata": {},
   "source": [
    "### Recompensa Promedio vs Pasos de Tiempo\n",
    "\n",
    "- **UCB1 con `c = 0.01`** alcanza la mayor recompensa promedio (~0.84), lo que indica rápida convergencia al brazo óptimo con poca exploración.\n",
    "- **UCB1 con `c = 1.0`** muestra un rendimiento final similar a **`c = 0.01`** aunque con más exploración.\n",
    "- **UCB1 con `c = 3.0`** muestra menor rendimiento (alrededor de 0.7), indicando que la exploración más intensa les hizo pasar más tiempo en brazos subóptimos.\n",
    "- **UCB2 con `alpha = 0.3`, `0.5`** logran un rendimiento bueno, pero **`alpha = 0.01`** sufre una caída notable a partir de los 400 pasos, señal de que converge a un brazo erróneo.\n",
    "\n",
    "Este gráfico muestra cómo una **exploración excesiva** o **mal dirigida** puede perjudicar en problemas con alta varianza y recompensas discretas como en el caso Bernoulli.\n",
    "\n",
    "---\n",
    "\n",
    "### Porcentaje de Selección del Brazo Óptimo\n",
    "\n",
    "- **UCB1 con `c = 0.01`** y `c = 1.0` se aproximan al **50%** de selección del óptimo.\n",
    "- **UCB1 con `3.0`** converge peor, alrededor del **30%**.\n",
    "- **UCB2 con `alpha = 0.01`** comienza bien pero luego cae drásticamente.\n",
    "- **UCB2 con `alpha = 0.3` y `0.5`** se estabilizan cerca del **50–60%**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Regret Acumulado vs Pasos de Tiempo\n",
    "\n",
    "- **UCB1 con `c = 0.01`** mantiene el **regret más bajo y estable**.\n",
    "- **UCB1 con `c = 3.0`** y **UCB2 con `alpha = 0.01`** muestran **regret alto (disminuyendo lentamente)** y **regret creciente** respectivamente, indicativo de convergencia a un brazo subóptimo.\n",
    "- **UCB2 con `alpha = 0.3` y `0.5`** logran un **regret bajo pero más oscilante**.\n",
    "\n",
    "La dificultad para discriminar el óptimo en Bernoulli (especialmente con valores de `p` cercanos) se refleja aquí: más exploración **no siempre mejora el regret** si no es eficaz.\n",
    "\n",
    "---\n",
    "\n",
    "### Estadísticas de Selección y Recompensa por Brazo\n",
    "\n",
    "- **UCB1 con `c = 0.01`** se centra fuertemente en el brazo óptimo, aunque otros brazos también fueron explorados bastante.\n",
    "- **UCB1 con `c = 3.0`** distribuye las elecciones de forma más amplia, penalizando la explotación.\n",
    "- **UCB2 con `alpha = 0.01`** muestra una elección excesiva de un brazo subóptimo (fallo de convergencia).\n",
    "- **UCB2 con `alpha = 0.3` y `0.5`** muestran distribución equilibrada pero sin una clara dominancia del óptimo (similar a UCB1 con `c = 0.01`).\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "En problemas con recompensas **binarias y ruidosas**, como Bernoulli, una **exploración demasiado intensa puede impedir la correcta identificación del óptimo**. UCB1 con una exploración mínima (`c = 0.01`) y UCB2 (**`alpha = 0.3` y `0.5`**) parece ser la estrategia más efectiva en este entorno.\n",
    "\n",
    "\n",
    "Para entornos discretos y ruidosos, **menos exploración puede ser mejor** si el brazo óptimo no es difícil de encontrar desde el inicio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5facc98-0b36-42b4-b237-a9abd41413b8",
   "metadata": {},
   "source": [
    "## Visualización de los resultados (Distribución Binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367531a0-08b3-4318-b061-51aba5e2d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards_binomial, algorithms)\n",
    "plot_optimal_selections(steps, optimal_selections_binomial, algorithms)\n",
    "plot_regret(steps, regret_accumulated_binomial, algorithms)\n",
    "plot_arm_statistics(arm_stats_binomial, algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d441918-1eb3-4678-ba4b-5e5e97b8fa64",
   "metadata": {},
   "source": [
    "### Recompensa Promedio vs Pasos de Tiempo\n",
    "\n",
    "- Todos los algoritmos convergen muy rápidamente a una recompensa promedio **alta (~12)**.\n",
    "- No se observan diferencias significativas entre configuraciones de `c` o `alpha` en este aspecto (aunque se aprecian ciertas oscilaciones para el caso de UCB1(`c = 3.0`)).\n",
    "\n",
    "---\n",
    "\n",
    "### Porcentaje de Selección del Brazo Óptimo\n",
    "\n",
    "- Todos los algoritmos alcanzan un **porcentaje de selección del óptimo cercano al 100%**.\n",
    "- Tanto UCB1 como UCB2 logran identificar el brazo óptimo en muy pocas iteraciones y se mantienen explotándolo de forma estable.\n",
    "\n",
    "---\n",
    "\n",
    "### Regret Acumulado vs Pasos de Tiempo\n",
    "\n",
    "- El **regret inicial** es visible en los primeros pasos, pero cae rápidamente a **casi cero** en todos los algoritmos.\n",
    "- No hay diferencias significativas entre configuraciones: todas tienen un comportamiento muy eficiente.\n",
    "- A partir de los 200 pasos, el regret prácticamente desaparece.\n",
    "\n",
    "Este resultado confirma que el entorno binomial, con mayor información por muestra, permite una identificación más rápida del brazo óptimo.\n",
    "\n",
    "---\n",
    "\n",
    "### Estadísticas de Selección y Recompensa por Brazo\n",
    "\n",
    "- El **brazo óptimo** es claramente el más seleccionado por todos los algoritmos (alrededor de 490.000 selecciones de 500.000).\n",
    "- Todos los demás brazos fueron seleccionados muy pocas veces (500–1,200), principalmente durante las fases iniciales de exploración.\n",
    "- Las recompensas promedio reflejan adecuadamente la media esperada de cada brazo, mostrando que los algoritmos realizaron una buena estimación.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "En problemas con **recompensas discretas, pero bien diferenciadas y estables** como los binomiales, tanto UCB1 como UCB2 funcionan de forma excelente. Todos los parámetros probados alcanzan rápidamente el óptimo y presentan un **regret muy bajo**.\n",
    "\n",
    "Este experimento demuestra que, cuando la señal es clara y la varianza es baja, **todas las configuraciones de UCB convergen eficientemente**. No se requieren ajustes finos de los hiperparámetros para lograr un rendimiento óptimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d14c52-ae35-46bb-aa1b-89b845076d57",
   "metadata": {},
   "source": [
    "## Conclusión general\n",
    "\n",
    "- UCB es robusto en Gaussianas y Binomiales.\n",
    "- En Bernoulli, la sobre-exploración penalizó.\n",
    "- UCB1 c=1.0 y UCB2 α=0.3 fueron los mejores globales."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
